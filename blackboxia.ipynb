{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c6359f",
   "metadata": {},
   "source": [
    "El error indica que el archivo del modelo GPT4All no se encuentra en la ruta esperada:\n",
    "\n",
    "```\n",
    "FileNotFoundError: Model file does not exist: WindowsPath('C:/Users/iparra/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Cómo resolverlo paso a paso\n",
    "\n",
    "### 1. Descargar el modelo GPT4All\n",
    "\n",
    "- Ve a la página oficial de GPT4All:  \n",
    "  https://gpt4all.io/index.html  \n",
    "  o al repositorio oficial en GitHub:  \n",
    "  https://github.com/nomic-ai/gpt4all\n",
    "\n",
    "- Descarga el modelo `ggml-gpt4all-j-v1.3-groovy.bin` (o el que prefieras).\n",
    "\n",
    "### 2. Guardar el modelo en una ruta accesible\n",
    "\n",
    "- Puedes guardarlo en cualquier carpeta, por ejemplo:  \n",
    "  `C:/models/ggml-gpt4all-j-v1.3-groovy.bin`\n",
    "\n",
    "### 3. Modificar el código para apuntar a la ruta correcta\n",
    "\n",
    "En tu código, cambia la línea:\n",
    "\n",
    "```python\n",
    "llm = GPT4All(model=\"ggml-gpt4all-j-v1.3-groovy.bin\")\n",
    "```\n",
    "\n",
    "por la ruta completa donde guardaste el modelo, por ejemplo:\n",
    "\n",
    "```python\n",
    "llm = GPT4All(model=\"C:/models/ggml-gpt4all-j-v1.3-groovy.bin\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Verificar permisos y existencia\n",
    "\n",
    "- Asegúrate que el archivo existe en esa ruta.\n",
    "- Que tu usuario tenga permisos para leerlo.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Alternativa: usar un modelo diferente o ruta relativa\n",
    "\n",
    "Si prefieres mantener el modelo en la carpeta del proyecto, pon el archivo ahí y usa ruta relativa:\n",
    "\n",
    "```python\n",
    "llm = GPT4All(model=\"./ggml-gpt4all-j-v1.3-groovy.bin\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Resumen\n",
    "\n",
    "- Descarga el modelo `.bin`.\n",
    "- Guarda el archivo en una ruta conocida.\n",
    "- Cambia el parámetro `model` en `GPT4All()` para que apunte a esa ruta.\n",
    "- Ejecuta de nuevo.\n",
    "\n",
    "---\n",
    "\n",
    "Si quieres, puedo ayudarte a automatizar la descarga del modelo con Python o darte instrucciones para otros modelos libres. ¿Quieres?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "061c8533",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_community\\embeddings\\huggingface.py:84\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\__init__.py:15\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[0;32m     12\u001b[0m     export_optimized_onnx_model,\n\u001b[0;32m     13\u001b[0m     export_static_quantized_openvino_model,\n\u001b[0;32m     14\u001b[0m )\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     CrossEncoder,\n\u001b[0;32m     17\u001b[0m     CrossEncoderModelCardData,\n\u001b[0;32m     18\u001b[0m     CrossEncoderTrainer,\n\u001b[0;32m     19\u001b[0m     CrossEncoderTrainingArguments,\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:16\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautonotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     AutoConfig,\n\u001b[0;32m     18\u001b[0m     AutoModelForSequenceClassification,\n\u001b[0;32m     19\u001b[0m     AutoTokenizer,\n\u001b[0;32m     20\u001b[0m     PretrainedConfig,\n\u001b[0;32m     21\u001b[0m     PreTrainedModel,\n\u001b[0;32m     22\u001b[0m     PreTrainedTokenizer,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:2302\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2301\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2302\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2303\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:2332\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:2330\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2329\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py:53\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdpa_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sdpa_attention_forward\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LOSS_MAPPING\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     Conv1D,\n\u001b[0;32m     56\u001b[0m     apply_chunking_to_forward,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m     translate_to_torch_parallel_style,\n\u001b[0;32m     63\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\loss\\loss_utils.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, MSELoss\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_deformable_detr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_for_object_detection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ForObjectDetectionLoss, ForSegmentationLoss\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\loss\\loss_deformable_detr.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_to_corners_format\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_scipy_available\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\image_transforms.py:48\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\iparra\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 97\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Extraer metadata y crear índice (solo una vez)\u001b[39;00m\n\u001b[0;32m     96\u001b[0m metadata_texts \u001b[38;5;241m=\u001b[39m extraer_metadata_sqlite(db_path)\n\u001b[1;32m---> 97\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mcrear_vectorstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Consulta de ejemplo\u001b[39;00m\n\u001b[0;32m    100\u001b[0m consulta \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMuéstrame los nombres y correos de los clientes que han comprado música en 2010\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[19], line 31\u001b[0m, in \u001b[0;36mcrear_vectorstore\u001b[1;34m(metadata_texts)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrear_vectorstore\u001b[39m(metadata_texts):\n\u001b[1;32m---> 31\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall-MiniLM-L6-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     vectorstore \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_texts(metadata_texts, embeddings)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vectorstore\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\_api\\deprecation.py:223\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     emit_warning()\n\u001b[1;32m--> 223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_community\\embeddings\\huggingface.py:87\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer(\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, cache_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs\n\u001b[0;32m     94\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`."
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 1. Extraer metadata de Chinook.db\n",
    "def extraer_metadata_sqlite(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Obtener tablas\n",
    "    cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tablas = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "    metadata = []\n",
    "    for tabla in tablas:\n",
    "        # Columnas y tipos\n",
    "        cur.execute(f\"PRAGMA table_info({tabla});\")\n",
    "        columnas = cur.fetchall()  # (cid, name, type, notnull, dflt_value, pk)\n",
    "        columnas_str = \", \".join([f\"{col[1]} ({col[2]})\" for col in columnas])\n",
    "        metadata.append(f\"Tabla {tabla}: {columnas_str}\")\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return metadata\n",
    "\n",
    "# 2. Crear índice vectorial con metadata\n",
    "def crear_vectorstore(metadata_texts):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_texts(metadata_texts, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "# 3. Configurar LLM local GPT4All\n",
    "# llm = GPT4All(model=\"ggml-gpt4all-j-v1.3-groovy.bin\")  # Ajusta el path a tu modelo\n",
    "# llm = GPT4All(model=\"c:/Users/iparra/gpt4all/models/ggml-gpt4all-l13b-snoozy.bin\")\n",
    "# Ruta local a un .gguf (ejemplo)\n",
    "model_path = r\"C:\\Users\\iparra\\gpt4all\\models\\mistral-7b-instruct-v0.2.Q4_0.gguf\"\n",
    "llm = GPT4All(model=model_path, allow_download=False)  # pon True si querés que descargue\n",
    "\n",
    "# 4. Prompt template\n",
    "template = \"\"\"\n",
    "Eres un asistente que convierte lenguaje natural a SQL para SQLite.\n",
    "Metadata relevante:\n",
    "{metadata}\n",
    "\n",
    "Consulta:\n",
    "{query}\n",
    "\n",
    "Genera la consulta SQL.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"metadata\", \"query\"], template=template)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# 5. Ejecutar SQL en SQLite\n",
    "def ejecutar_sql_sqlite(db_path, sql_query):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        cur.execute(sql_query)\n",
    "        # Intentar obtener resultados si es SELECT\n",
    "        if sql_query.strip().lower().startswith(\"select\"):\n",
    "            result = cur.fetchall()\n",
    "        else:\n",
    "            conn.commit()\n",
    "            result = \"Consulta ejecutada correctamente.\"\n",
    "    except Exception as e:\n",
    "        result = f\"Error al ejecutar SQL: {e}\"\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return result\n",
    "\n",
    "# 6. Función principal\n",
    "def text_to_sql_chinook(db_path, query_usuario, vectorstore):\n",
    "    # Buscar metadata relevante\n",
    "    docs = vectorstore.similarity_search(query_usuario, k=3)\n",
    "    metadata_relevante = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    # Generar SQL\n",
    "    sql = chain.run(metadata=metadata_relevante, query=query_usuario)\n",
    "    print(\"SQL generado:\\n\", sql)\n",
    "\n",
    "    # Ejecutar SQL\n",
    "    resultado = ejecutar_sql_sqlite(db_path, sql)\n",
    "    print(\"Resultado:\\n\", resultado)\n",
    "\n",
    "    return resultado\n",
    "\n",
    "# --- Uso ---\n",
    "if __name__ == \"__main__\":\n",
    "    db_path = \"Chinook.db\"  # Ruta a tu base Chinook.db\n",
    "\n",
    "    # Extraer metadata y crear índice (solo una vez)\n",
    "    metadata_texts = extraer_metadata_sqlite(db_path)\n",
    "    vectorstore = crear_vectorstore(metadata_texts)\n",
    "\n",
    "    # Consulta de ejemplo\n",
    "    consulta = \"Muéstrame los nombres y correos de los clientes que han comprado música en 2010\"\n",
    "    respuesta = text_to_sql_chinook(db_path, consulta, vectorstore)\n",
    "    print(\"\\nRespuesta final:\\n\", respuesta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "580fe50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autogluon-common 1.3.1 has requirement numpy<2.3.0,>=1.25.0, but you have numpy 2.3.3.\n",
      "autogluon-core 1.3.1 has requirement numpy<2.3.0,>=1.25.0, but you have numpy 2.3.3.\n",
      "autogluon-features 1.3.1 has requirement numpy<2.3.0,>=1.25.0, but you have numpy 2.3.3.\n",
      "autogluon-multimodal 1.3.1 has requirement numpy<2.3.0,>=1.25.0, but you have numpy 2.3.3.\n",
      "autogluon-tabular 1.3.1 has requirement numpy<2.3.0,>=1.25.0, but you have numpy 2.3.3.\n",
      "autogluon-timeseries 1.3.1 has requirement numpy<2.3.0,>=1.25.0, but you have numpy 2.3.3.\n",
      "datasets 3.6.0 has requirement fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0.\n",
      "gluonts 0.16.1 has requirement numpy<2.2,>=1.16, but you have numpy 2.3.3.\n",
      "lightning 2.5.1.post0 has requirement packaging<25.0,>=20.0, but you have packaging 25.0.\n",
      "neuralprophet 0.8.0 has requirement numpy<2.0.0,>=1.25.0, but you have numpy 2.3.3.\n",
      "node2vec 0.5.0 has requirement numpy<2.0.0,>=1.24.0, but you have numpy 2.3.3.\n",
      "sktime 0.37.0 has requirement numpy<2.3,>=1.21, but you have numpy 2.3.3.\n",
      "tensorflow 2.19.0 has requirement numpy<2.2.0,>=1.26.0, but you have numpy 2.3.3.\n",
      "tensorflow 2.19.0 has requirement protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.0.\n",
      "aiobotocore 2.12.3 has requirement botocore<1.34.70,>=1.34.41, but you have botocore 1.38.27.\n",
      "contourpy 1.2.0 has requirement numpy<2.0,>=1.20, but you have numpy 2.3.3.\n",
      "numba 0.59.1 has requirement numpy<1.27,>=1.22, but you have numpy 2.3.3.\n",
      "pywavelets 1.5.0 has requirement numpy<2.0,>=1.22.4, but you have numpy 2.3.3.\n",
      "s3fs 2024.3.1 has requirement fsspec==2024.3.1, but you have fsspec 2025.9.0.\n",
      "streamlit 1.32.0 has requirement numpy<2,>=1.19.3, but you have numpy 2.3.3.\n",
      "streamlit 1.32.0 has requirement packaging<24,>=16.8, but you have packaging 25.0.\n",
      "streamlit 1.32.0 has requirement protobuf<5,>=3.20, but you have protobuf 6.32.0.\n"
     ]
    }
   ],
   "source": [
    "# !pip install \"transformers[sentencepiece]>=4.38,<4.50\" --upgrade --upgrade-strategy eager\n",
    "!pip check   # verifica que ya no haya conflictos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dc63355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I’m back with another post about my travels in Spain. Today we are going to visit the beautiful city of Granada and its famous Alhambra Palace!\n",
      "\n",
      "Granada is located in Andalusia, southern Spain, and it’s known for being home to one of the most visited tourist attractions in Spain: The Alhambra Palace. This stunning palace complex was built during the Nasrid Dynasty (1232-1492) and it’s a perfect example of Islamic art and architecture.\n",
      "\n",
      "To get there, I took an early train from Málaga to Granada. It only takes about 2 hours and the trains are quite comfortable. Once you arrive in Granada, you can easily reach the Alhambra by taking bus number C3 or C4 from the Gran Vía stop (just a few minutes walk from the train station).\n",
      "\n",
      "The entrance fee to visit the Alhambra is €14 for adults and it’s recommended that you book your tickets online in advance, as they sell out quickly. Keep in mind that there are different visiting hours depending on the season, so make sure to check their website before planning your trip.\n",
      "\n",
      "When\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "# Ruta local a un .gguf (ejemplo)\n",
    "model_path = r\"C:\\Users\\iparra\\gpt4all\\models\\mistral-7b-instruct-v0.2.Q4_0.gguf\"\n",
    "\n",
    "llm = GPT4All(model=model_path, allow_download=False)  # pon True si querés que descargue\n",
    "print(llm.invoke(\"Hola, ¿cómo estás?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
